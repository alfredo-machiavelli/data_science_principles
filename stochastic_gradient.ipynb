{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzWJ0hrg7z77"
      },
      "source": [
        "## EE 461P: Data Science Principles  \n",
        "### Assignment 3 \n",
        "### Total points: 90\n",
        "### Due: Tuesday, Mar 8, 2022, submitted via Canvas by 11:59 pm  \n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "### Name(s) and EID(s):\n",
        "1. Harshika Jha hj6963\n",
        "2. Jared McArthur jtm4343\n",
        "\n",
        "### Homework group No.: "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 Stochastic Gradient Descent - (20 pts)\n",
        "(a). (**10 pts**) Using stochastic gradient descent, derive the coefficent updates for all 6 weights of the model (assuming squared error as your loss function): \n",
        "$$ y = w_0 + w_1x_{1} + w_2x_{2} + w_3x_{1}x_{2} + w_4e^{-x_{1}x_{2}}+ w_5log_{e}(x_{3})$$ \n",
        "where $(x_i,t_i)$ is the $i$-th point in the data set\n",
        "\n",
        "  Note: For a single point $(x_i,t_i)$, in the given dataset, let:\n",
        "  \\begin{equation}\n",
        "  E_i(w) =  t_i -(w_0 + w_1x_{i1} + w_2x_{i2} + w_3x_{i1}x_{i2} + w_4e^{-x_{i1}x_{i2}}+ w_5log_{e}(x_{i3}))\n",
        "  \\end{equation}\n",
        "  Express the partial derivatives in your update equations in the form of $E_i(w)$, $x_{i1}$,$x_{i2}$, and $x_{i3}$\n",
        "\n",
        "\n",
        "(b) (**10 pts**)Download the x and t data arrays  from the following links respectively (There are 2000 points in total)\n",
        "https://drive.google.com/file/d/1xhXhGmsLCBYqVmUAXktFemyNwee9HNF4/view?usp=sharing\n",
        "\n",
        "\n",
        "https://drive.google.com/file/d/1--2vBshmb3Havi4kuSWo5A-t-WAxWcc1/view?usp=sharing\n",
        "\n",
        "Initialize the weights from the zero mean normal distribution with standard deviation of 3. Set the learning rate to $10^{-4}$. For 100 epochs (An epoch is completed when we have passed through the entire training dataset), learn the weights using stochastic gradient descent.You are only allowed to use the numpy and matplotlib libraries.  Calculate the MSE at the end of each epoch and plot it against the epochs. What are the weights that are learned? (You do not need to separate data in training set and testing sets)"
      ],
      "metadata": {
        "id": "7Spp8KYdtf3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer 1 (a)."
      ],
      "metadata": {
        "id": "kIdSENppuahz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1BmiB8SIjRr_qDCzXcygc97PnXlyX6xNP/view?usp=sharing"
      ],
      "metadata": {
        "id": "ClhqhBbfOsRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer 1 (b)"
      ],
      "metadata": {
        "id": "6g5hPld9Q4ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.load('x.npy')\n",
        "t=np.load('y.npy')"
      ],
      "metadata": {
        "id": "pXztqiCQLuJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Usgc_rHHL7zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, learning_rate, regularization_constant, num_epochs):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.regularization_constant = regularization_constant\n",
        "        self.num_epochs = num_epochs\n",
        "        self.coef_ = [0, 0, 0, 0]\n",
        "        self.coefficients = []\n",
        "        self.mses = []\n",
        "        self.losses = 0\n",
        "        \n",
        "    def fit(self, X, y, update_rule):\n",
        "        x1 = X['x_1']\n",
        "        x2 = X['x_2']\n",
        "        x1x2 = X['x_1'] * X['x_2']\n",
        "        \n",
        "        cache = [0, 0, 0, 0]\n",
        "        for epoch in range(self.num_epochs):\n",
        "            loss = []\n",
        "            for i in range(len(x1)):\n",
        "                predictY = self.coef_[0] + self.coef_[1]*x1[i] + self.coef_[2]*x1x2[i] + self.coef_[3]*x2[i]\n",
        "                                \n",
        "                grad = [0,0,0,0]\n",
        "                grad[0] = predictY - y[i] + self.coef_[0]\n",
        "                grad[1] = grad[0]*x1[i] + self.coef_[1]\n",
        "                grad[2] = grad[0]*x1x2[i] + self.coef_[2]\n",
        "                grad[3] = grad[0]*x2[i] + self.coef_[3]\n",
        "                \n",
        "                if(update_rule is \"adagrad\"):\n",
        "                    cache[0] += grad[0]**2\n",
        "                    cache[1] += grad[1]**2\n",
        "                    cache[2] += grad[2]**2\n",
        "                    cache[3] += grad[3]**2\n",
        "                \n",
        "                    self.coef_[0] = self.coef_[0] - (self.learning_rate)/np.sqrt(cache[0] + 1e-6) * (grad[0] + self.regularization_constant * self.coef_[0])\n",
        "                    self.coef_[1] = self.coef_[1] - (self.learning_rate)/np.sqrt(cache[1] + 1e-6) * (grad[1] + self.regularization_constant * self.coef_[1])\n",
        "                    self.coef_[2] = self.coef_[2] - (self.learning_rate)/np.sqrt(cache[2] + 1e-6) * (grad[2] + self.regularization_constant * self.coef_[2])\n",
        "                    self.coef_[3] = self.coef_[3] - (self.learning_rate)/np.sqrt(cache[3] + 1e-6) * (grad[3] + self.regularization_constant * self.coef_[3])\n",
        "                    \n",
        "                elif update_rule is \"SGD\":\n",
        "                    self.coef_[0] = self.coef_[0] - self.learning_rate * (grad[0] + self.regularization_constant * self.coef_[0])\n",
        "                    self.coef_[1] = self.coef_[1] - self.learning_rate * (grad[1] + self.regularization_constant * self.coef_[1])\n",
        "                    self.coef_[2] = self.coef_[2] - self.learning_rate * (grad[2] + self.regularization_constant * self.coef_[2])\n",
        "                    self.coef_[3] = self.coef_[3] - self.learning_rate * (grad[3] + self.regularization_constant * self.coef_[3])\n",
        "\n",
        "                loss.append((y[i]-predictY)**2 )\n",
        "            mse = np.mean(np.array(loss))\n",
        "            self.mses.append(mse)\n",
        "            \n",
        "            if epoch is self.num_epochs - 1:\n",
        "                self.losses = mse\n",
        "    \n",
        "    def predict(self, X):\n",
        "        x1 = X['x_1']\n",
        "        x2 = X['x_2']\n",
        "        x1x2 = X['x_1'] * X['x_2']\n",
        "\n",
        "        y = self.coef_[0] + self.coef_[1]*x1 + self.coef_[2]*x1x2 + self.coef_[3]*x2\n",
        "\n",
        "        return y "
      ],
      "metadata": {
        "id": "z48a9y82MiJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx = x.to_csv(x)     \n",
        "df = pd.read_csv(dx)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['x_1', 'x_2']], df['y'], test_size = 0, random_state = 10)\n",
        "\n",
        "\n",
        "losses_sgd = []\n",
        "mses_sgd = []\n",
        "regConsts_sgd = []\n",
        "learningRates_sgd = []\n",
        "num_epochs_sgd = 10\n",
        "for learning_rate in [.0001, .001, .01, 0.1, 1, 10, 100]:\n",
        "    for regularization_constant in [0,10,100]:\n",
        "        mySGD = SGD(learning_rate, regularization_constant, 10)\n",
        "        mySGD.fit(X_train, y_train, \"SGD\")\n",
        "        #predictedY = mySGD.predict(X_test)\n",
        "        losses_sgd.append(mySGD.losses)\n",
        "        mses_sgd.append(mySGD.mses)\n",
        "        regConsts_sgd.append(regularization_constant)\n",
        "        learningRates_sgd.append(learning_rate)\n",
        "\n",
        "print(\"SGD:\")\n",
        "print(\"Smallest mse:\", min(losses_sgd), \"reg const:\", regConsts_sgd[losses_sgd.index(min(losses_sgd))],\"learning rate:\", learningRates_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "\n"
      ],
      "metadata": {
        "id": "NWIhms_vMm-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(10), mses_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "plt.title('MSE plots against epoches count for best SGD')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()\n",
        "\n",
        "regConsts_sgd.remove(regConsts_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "learningRates_sgd.remove(learningRates_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "losses_sgd.remove(min(losses_sgd))\n",
        "mses_sgd.remove(mses_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "\n",
        "print(\"Second smallest mse:\", min(losses_sgd), \"reg const:\", regConsts_sgd[losses_sgd.index(min(losses_sgd))],\"learning rate:\", learningRates_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(10), mses_sgd[losses_sgd.index(min(losses_sgd))])\n",
        "plt.title('MSE plots against epoches count for second best SGD')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T2rglqU1Mskk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 - Activation functions (10 points)\n",
        "The Rectified Linear unit (ReLU) function is defined as\n",
        "$$f(u)=max(0,u)$$ \n",
        "and\n",
        "\n",
        "Sigmoid activation function is defined as \n",
        "$$f(u)=\\frac{1}{1+e^{-u}}$$ \n",
        "\n",
        "(a) (5 points) Let $\\psi(u)=ReLU(-ReLU(u))$. Suppose we use this as the activation function employed for  one of the hidden layers, what is expected to happen to the output of a Neural network? \n",
        "\n",
        "(b) (5 points) Let our target variable in a regression problem be uniformally distributed in the range -10 to 10. What would be the limitation of the model if the sigmoid activation function is used for  the output cell of a Neural Network that aims to train on such data?"
      ],
      "metadata": {
        "id": "_Lbf7ENn7wJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer 2"
      ],
      "metadata": {
        "id": "4FwEYcUzXv3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) The gradient value becomes 0 on the negative and positive sides of the graph. \n",
        "\n",
        "(b) output of all neurons must be the same sign (positive or negative); not symmetric around 0"
      ],
      "metadata": {
        "id": "1uMCIK328__e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 : Backpropagation and Neural Networks (20 points)\n",
        "\n",
        "Consider the neural network shown in the figure. The input to the network is a 2-dimensional vector $(x_{1}, x_{2})$ and there is no bias term. The network has 2 layers (note that there is only one hidden unit) and 3 weights - $w_{1}$ , $w_{2}$ and $w_{3}$  along with a sigmoid activation function $s(\\alpha) = \\frac{1}{1 + e^{-\\alpha}}$.\n",
        "\n",
        "![Toy Neural Network (2).jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "The expression for the output $y$ of the network can be written as -\n",
        "\n",
        "$$ t = w_{3} * s(w_{1}*x_{1} + w_{2}*x_{2}) $$\n",
        "\n",
        "Suppose that the true label for this particular input was supposed to be $y$, leading to the squared prediction error $ L = (t - y)^{2} $ (Here $t$ represents the output of the network). \n",
        "\n",
        "Derive the update equations for the following two  weights \n",
        "\n",
        "a) $w_{3}$ (5 points)\n",
        "\n",
        "b) $w_{1}$ (10 points)\n",
        "\n",
        "backpropagation is performed based on this error, using the \n",
        "learning rate $\\eta$."
      ],
      "metadata": {
        "id": "XjablRFHhDi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer 3."
      ],
      "metadata": {
        "id": "r9hqFDT-o4-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1YuocPF-VAzZKIrPmjc2EMiLfP_yDMZVp/view?usp=sharing"
      ],
      "metadata": {
        "id": "JYwclHi7OvEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 : Pre-processing and MLP Regression (30 points)\n",
        "\n",
        "Suppose you are the owner of an online business. You want to consider displaying advertisements on your website to increase your revenue.\n",
        "\n",
        "There are 5 different agencies you can consider - Agency A, B, C, D and E. Each of these agencies display different type of advertisements, and offer different revenue models. The revenue is governed by the click-through-rate (number of clicks per unit hour) that your website achieves for different advertisements.\n",
        "\n",
        "As a data scientist, you decide to conduct an experiment using the different available agencies to decide which one is the best, by showing different advertisments for a 1-hour window each. Through the logs of the website, you gather the following data (Each entry represents information about a click made on an advertisement on the website) - \n",
        "\n",
        "1. `advertisement_index` : An identifier for the advertisement being displayed\n",
        "2. `time_of_day` : The time of the day at which the advertisement was clicked\n",
        "3. `day_of_week` : The day of the week on which the advertisement was clicked\n",
        "4. `agency`      : The agency which was responsible for the advertisement being displayed\n",
        "5. `text_image_ratio` : The ratio of text to image on the advertisement being displayed\n",
        "6. `number_of_users` : The number of users who have visited on the day until this click\n",
        "\n",
        "You can find the data required for Problem 4 [here](https://drive.google.com/drive/folders/1BIGdwGFkjNALdHsv-X-UPTTzv_tOOoFx?usp=sharing). You should first download the data from the drive, and upload it on the Google Colab. "
      ],
      "metadata": {
        "id": "e_9RLzJBzhy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Common Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
      ],
      "metadata": {
        "id": "E2RFaeuYFiAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data_problem4.csv', parse_dates=True)\n",
        "data['time_of_day'] = pd.to_datetime(data['time_of_day'])"
      ],
      "metadata": {
        "id": "BDVWW6KRgZnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. [7 points] Pre-processing"
      ],
      "metadata": {
        "id": "FlJzzP7CleF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) [1 points] Print out the first 5 rows of the data."
      ],
      "metadata": {
        "id": "BLgeUZmyuxnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  print(data.loc[i,:])"
      ],
      "metadata": {
        "id": "odvSm1sEKL7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) [5 points] You will notice that we don't have a column labelled as the `click_through_rate` in the data, based on which you are actually supposed to make your decision for the best agency. However, you do have information about individual clicks. \n",
        "\n",
        "To get an estimate for the `click_through_rate` you decide to group the clicks by discretising them in bins of 1 hour each - \n",
        "{ (0-1), (1-2) ... (23-24) } (The times are represented in 24 hr format).\n",
        "\n",
        "\n",
        "For this problem, create a new dataframe with the following columns - \n",
        "\n",
        "1. `advertisement_index` : An identifier for the advertisement being displayed\n",
        "2. `start_time` : The start time of the bin (Should be a value from {0, 1, 2 ... 24})\n",
        "3. `day_of_week` : The day of the week on which the advertisement was being displayed\n",
        "4. `number_of_clicks` : The number of clicks in the duration - (`start_time`, `start_time` + 1)\n",
        "5. `agency` : The agency which was responsible for the advertisement being displayed\n",
        "6. `text_image_ratio` : The ratio of text to image on the advertisement being displayed\n",
        "7. `avg_number_of_users` : The average number of users who visited the website in the duration of (`start_time`, `start_time` + 1)\n",
        "\n",
        "\n",
        "(Note : There can be multiple ways to do this. The most straightforward one I could think of is - you can just iterate through the data row-by-row, consider entries with the same `advertisement_index` and divide them into the required bins. You can assume that each advertisement was only shown for a 1-hour window, so the start of the bin would just be the hour component of the time.)"
      ],
      "metadata": {
        "id": "_ah8a3weliyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ad_dict = {\n",
        "    \"advertisement_index\": [],\n",
        "    \"start_time\": [],\n",
        "    \"day_of_week\": [],\n",
        "    \"number_of_clicks\": [],\n",
        "    \"agency\": [],\n",
        "    \"text_image_ratio\": [],\n",
        "    \"avg_number_of_users\": []\n",
        "}\n",
        "\n",
        "# tuple of info: (agency, text_image_ratio)\n",
        "info_dict = {}\n",
        "# ad_index -> day of week -> hour\n",
        "num_users_dict = {}\n",
        "clicks_dict = {}\n",
        "for i in range(len(data)):\n",
        "  ad_index = data.loc[i].at[\"advertisment_index\"]\n",
        "  day_of_week = data.loc[i].at[\"day_of_week\"]\n",
        "  hour = data.loc[i].at[\"time_of_day\"].hour\n",
        "  num_users = data.loc[i].at[\"number_of_users\"]\n",
        "  agency = data.loc[i].at[\"agency\"]\n",
        "  text_image_ratio = data.loc[i].at[\"text_image_ratio\"]\n",
        "\n",
        "  if (clicks_dict.get(ad_index) == None):\n",
        "    clicks_dict[ad_index] = {}\n",
        "    num_users_dict[ad_index] = {}\n",
        "  if (clicks_dict.get(ad_index).get(day_of_week) == None):\n",
        "    clicks_dict.get(ad_index)[day_of_week] = {}\n",
        "    num_users_dict.get(ad_index)[day_of_week] = {}\n",
        "  \n",
        "  if (clicks_dict.get(ad_index).get(day_of_week).get(hour) == None):\n",
        "    clicks_dict.get(ad_index).get(day_of_week)[hour] = 1\n",
        "    num_users_dict.get(ad_index).get(day_of_week)[hour] = []\n",
        "  else:\n",
        "    clicks_dict.get(ad_index).get(day_of_week)[hour] = clicks_dict.get(ad_index).get(day_of_week).get(hour) + 1\n",
        "\n",
        "  num_users_dict.get(ad_index).get(day_of_week).get(hour).append(num_users)\n",
        "  info_dict[ad_index] = (agency, text_image_ratio)\n",
        "\n",
        "# avg num users is the diff btwn the max for that time and day since that is the number of users that accessed during that time.\n",
        "# at least according to the description of number_of_users\n",
        "for ad_index, ad_index_days in clicks_dict.items():\n",
        "  for day, hours_in_day in ad_index_days.items():\n",
        "    for hour, clicks in hours_in_day.items():\n",
        "      ad_dict[\"advertisement_index\"].append(ad_index)\n",
        "      ad_dict[\"start_time\"].append(hour)\n",
        "      ad_dict[\"day_of_week\"].append(day)\n",
        "      ad_dict[\"number_of_clicks\"].append(clicks)\n",
        "      ad_dict[\"agency\"].append(info_dict.get(ad_index)[0])\n",
        "      ad_dict[\"text_image_ratio\"].append(info_dict.get(ad_index)[1])\n",
        "      ad_dict[\"avg_number_of_users\"].append(max(num_users_dict.get(ad_index).get(day).get(hour)) - min(num_users_dict.get(ad_index).get(day).get(hour)))\n",
        "\n",
        "new_df = pd.DataFrame.from_dict(ad_dict)"
      ],
      "metadata": {
        "id": "vsaOi2pGNeoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) [1 points] Print out the shape of the new dataframe (Expected to be (2979,7)) and the first 5 rows."
      ],
      "metadata": {
        "id": "QlFWq90Wb1NV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since we have bucketed by 1-hour windows, the number of clicks in that window can be treated as a measure of the `click-through-rate`. Therefore, we are going to use the `number_of_clicks` as our response variable in the subsequent parts of the question to find the best advertising agency. "
      ],
      "metadata": {
        "id": "HnA99w_1cOAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_df.shape)\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print(new_df.loc[i,:])"
      ],
      "metadata": {
        "id": "dtdwM-RaD76k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. [5 points] Visualisation"
      ],
      "metadata": {
        "id": "W7QLJESjcnJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) [1 points] Plot a graph of the Average `number_of_clicks` vs the `start_time` and mention your observations. (For a particular value of the `start_time`, average the `number_of_clicks`.) [Use the [pandas.groupby(.)](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function to help with aggregation]"
      ],
      "metadata": {
        "id": "DDd1ihpYNJia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clicks_v_start_time = new_df.groupby(\"start_time\").agg(avg_num_clicks=(\"number_of_clicks\", np.mean))\n",
        "\n",
        "plt.plot(clicks_v_start_time)"
      ],
      "metadata": {
        "id": "-rb3bGIqNT08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) [1 points] Plot a graph of the Average `number_of_clicks` vs the `day_of_week` and mention your observations. (For a particular value of the `day_of_week`, average the `number_of_clicks`.) [Use the [pandas.groupby(.)](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function to help with aggregation]\n",
        "\n",
        "[Hint : Order the `day_of_week` from Monday to Sunday to observe a neat trend]"
      ],
      "metadata": {
        "id": "EbRkP666XToX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clicks_v_day_of_week = new_df.groupby(\"day_of_week\").agg(avg_num_clicks=(\"number_of_clicks\", np.mean))\n",
        "\n",
        "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "clicks_v_day_of_week.loc[day_order].plot(legend=False)"
      ],
      "metadata": {
        "id": "rLY9JiIPNUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) [1 points] Plot a graph of the Average `number_of_clicks` vs the `agency` and mention your observations. (For a particular value of the `agency`, average the `number_of_clicks`.) [Use the [pandas.groupby(.)](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function to help with aggregation]\n",
        "\n",
        "[Hint : Order the `agency` from Agency A to E to observe a neat trend]"
      ],
      "metadata": {
        "id": "Mdj8IQuSX27D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clicks_v_agency = new_df.groupby(\"agency\").agg(avg_num_clicks=(\"number_of_clicks\", np.mean))\n",
        "\n",
        "plt.plot(clicks_v_agency)"
      ],
      "metadata": {
        "id": "4ItJv2o3NVJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) [1 points] Divide the `text_image_ratio` into 10 equally-sized bins. Plot a graph of the average `number_of_clicks` in each bin vs the bins and mention your observations.\n",
        "\n",
        "[Hint : Use the [pandas.cut(.)](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) function to add a new column with bins, and the [pandas.groupby(.)](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function to help with aggregation]"
      ],
      "metadata": {
        "id": "fco5L_P-Zkzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"ratio_bins\"] = pd.qcut(new_df[\"text_image_ratio\"], q=10)\n",
        "\n",
        "clicks_v_ratio_bins = new_df.groupby(\"ratio_bins\").agg(avg_num_clicks=(\"number_of_clicks\", np.mean))\n",
        "\n",
        "plt.plot(clicks_v_ratio_bins)"
      ],
      "metadata": {
        "id": "QoQ-0xs8NVsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e) [1 points] Divide the `number_of_users` into 10 equally-sized bins. Plot a graph of the average `number_of_clicks` in each bin vs the bins and mention your observations.\n",
        "\n",
        "[Hint : Use the [pandas.cut(.)](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) function to add a new column with bins, and the [pandas.groupby(.)](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function to help with aggregation]"
      ],
      "metadata": {
        "id": "oObIt5qzIejI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"user_bins\"] = pd.qcut(new_df[\"avg_number_of_users\"], q=10, duplicates=\"drop\")\n",
        "\n",
        "clicks_v_user_bins = new_df.groupby(\"user_bins\").agg(avg_num_clicks=(\"number_of_clicks\", np.mean))\n",
        "\n",
        "plt.plot(clicks_v_user_bins)"
      ],
      "metadata": {
        "id": "xhpjzJpkNWLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. [5 points] Transformation"
      ],
      "metadata": {
        "id": "G8zZVY4GK0TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[5 points] Looking at the graphs in the previous section, you might have noticed that there does not exist a linear relationship between the predictor and response variables. Can you think of a transformation you can apply to the `number_of_clicks` so that the transformed variable and the predictor variables have a reasonably linear relationship?\n",
        "\n",
        "Create a new column in the data-frame with the transformed variable and plot all the graphs plotted in the previous section to verify that a linear relationship now exists."
      ],
      "metadata": {
        "id": "t2FTFTpRK5qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[\"transform\"] = np.where(True, 1 / new_df[\"number_of_clicks\"], 1 / new_df[\"number_of_clicks\"])\n",
        "\n",
        "clicks_v_start_time = new_df.groupby(\"start_time\").agg(transform=(\"transform\", np.mean))\n",
        "clicks_v_start_time.plot()\n",
        "\n",
        "\n",
        "clicks_v_day_of_week = new_df.groupby(\"day_of_week\").agg(transform=(\"transform\", np.mean))\n",
        "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "clicks_v_day_of_week.loc[day_order].plot(legend=False)\n",
        "\n",
        "clicks_v_agency = new_df.groupby(\"agency\").agg(transform=(\"transform\", np.mean))\n",
        "clicks_v_agency.plot()\n",
        "\n",
        "new_df[\"ratio_bins\"] = pd.qcut(new_df[\"text_image_ratio\"], q=10)\n",
        "clicks_v_ratio_bins = new_df.groupby(\"ratio_bins\").agg(transform=(\"transform\", np.mean))\n",
        "clicks_v_ratio_bins.plot()\n",
        "\n",
        "new_df[\"user_bins\"] = pd.qcut(new_df[\"avg_number_of_users\"], q=10, duplicates=\"drop\")\n",
        "clicks_v_user_bins = new_df.groupby(\"user_bins\").agg(transform=(\"transform\", np.mean))\n",
        "clicks_v_user_bins.plot()"
      ],
      "metadata": {
        "id": "49MTg4JjNWvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. [4 points] Feature Selection"
      ],
      "metadata": {
        "id": "9x_-a66adMMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) [2 points] Find the correlation matrix of the following variables and plot a heatmap.\n",
        "\n",
        "1. `start_time`\n",
        "5. `text_image_ratio`\n",
        "6. `avg_number_of_users`\n",
        "\n",
        "[Hint : See Homework 1]"
      ],
      "metadata": {
        "id": "yr68XVMlerw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = new_df[[\"start_time\", \"text_image_ratio\", \"avg_number_of_users\"]]\n",
        "\n",
        "his = sns.diverging_palette(100,100,as_cmap = True)\n",
        "sns.heatmap(data = x.corr())"
      ],
      "metadata": {
        "id": "tkQoBm2rNX5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) [2 points] Do you see anything weird in the matrix? Identify any variables that seem to be highly correlated to each other and eliminate one of them. \n",
        "\n",
        "In this matrix, nothing looks highly correlated. I assume this is because my avg number of users is incorrect due to the way I interpretted it."
      ],
      "metadata": {
        "id": "OUJB2l7chOdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. [2 points] Dummy Coding"
      ],
      "metadata": {
        "id": "NFw1D_K2lPuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use [pandas.get_dummies()](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to dummy code `day_of_week` and `agency`. Recall that categorical variables need to be dummy coded before they can be used in models."
      ],
      "metadata": {
        "id": "95nRlCP6oCSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummies = pd.get_dummies(new_df[[\"day_of_week\", \"agency\"]])"
      ],
      "metadata": {
        "id": "rWVfLraqNZCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. [7 points] Training"
      ],
      "metadata": {
        "id": "ehdzVrlHh4wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) [3 points] Ok so finally, after all our explorations, we are at the last stage. We are going to train an MLP model for our problem.\n",
        "\n",
        "Use the following set of variables as your predictor variables - \n",
        "\n",
        "1. `start_time`\n",
        "2. `day_of_week` (dummy coded)\n",
        "3. `agency` (dummy coded)\n",
        "4. `text_image_ratio`\n",
        "\n",
        "\n",
        "\n",
        "Using Multi-layer Perceptron regressor, fit a regression model with `alpha=0` on all the feature variables using a 80-20 split of the entire dataset. Report the total of number of weights present in the weight matrix (obtained using `model.coefs_`) and evaluate the model using mean squared error (MSE) on the test and training sets. An example is shown in [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)."
      ],
      "metadata": {
        "id": "cDQKUhNih8jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "9ofdXFZeNZz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) [2 points] Predict the number of clicks using the trained model for the training and test datasets. Don't forget to apply the inverse of the transformation you had applied previously. Plot a graph each for the training and test datasets of the average `number_of_clicks` and the average `predicted_number_of_clicks` vs the `start_time` and mention your observations. (For a particular value of the `start_time`, average the `number_of_clicks` and the `predicted_number_of_clicks`.) [Use the [pandas.groupby(.)](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups) function to help with aggregation]"
      ],
      "metadata": {
        "id": "GOqc2jXP6Oml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QtNGhORBNaZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) [2 points] Based on your model, let's say you are supposed to select an advertisement agency to display an advertisement on Wednesday afternoons from 12-1 PM. You ask the agencies to propose advertisements to bid for the slot. The values of the text_image_ratio of their proposed advertisements are given below - \n",
        "\n",
        "1. Agency A : 0.2\n",
        "2. Agency B : 0.25\n",
        "3. Agency C : 0.15\n",
        "4. Agency D : 0.3\n",
        "5. Agency E : 0.4\n",
        "\n",
        "Which agency would you select to maximise your `click-through-rate` on the advertisement during that 1 hour?"
      ],
      "metadata": {
        "id": "sZ2f-Lmgk63L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agency A"
      ],
      "metadata": {
        "id": "7mapX4UuQRmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5 : Sample Size Computation (10 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "RjaHUG4K332j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) (5 points) A manager of a supermarked needs to estimate the proportion of their customers that are vegan. Each day, a lot of people visit the store, so it is not possible to survey everyone. The manager decides to survey a smaller sample of the population to get an estimate. Since the true proportion of vegan customers is not known, it can be any value, even 50%!. The manager requests you to help him out. \n",
        "\n",
        "\n",
        "Determine the sample size necessary to estimate the proportion of people shopping at the supermarket that identify as vegan with 95% confidence, and a margin of error of 5%. "
      ],
      "metadata": {
        "id": "kkfbYmJN_gvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 5(a) 385"
      ],
      "metadata": {
        "id": "q5qZmn-QBQ6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) (3 points) Let's say that now we know that the true proportion of vegan customers can not exceed 10%. How many samples would now be required to estimate with 95% confidence and a 5% margin of error?"
      ],
      "metadata": {
        "id": "CVIJfJAPBZZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 5(b) 139"
      ],
      "metadata": {
        "id": "BkQzIuGRCoci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) (2 points) Further, after part b), let's say we reduce the margin of error to 2%, keeping everything else the same. What would the number of required samples be now?"
      ],
      "metadata": {
        "id": "8RNSEjLOCqJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 5(c) 865"
      ],
      "metadata": {
        "id": "99Pa-ZaXEDUP"
      }
    }
  ]
}